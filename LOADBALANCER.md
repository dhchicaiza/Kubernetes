# ğŸ”„ BALANCEADOR DE CARGA CON 2 PODS - GUÃA COMPLETA

## ğŸ“‹ Ãndice
1. [IntroducciÃ³n](#introducciÃ³n)
2. [Arquitectura](#arquitectura)
3. [Ajustes Detallados](#ajustes-detallados)
4. [Despliegue](#despliegue)
5. [Pruebas y VerificaciÃ³n](#pruebas-y-verificaciÃ³n)
6. [Escalado AutomÃ¡tico](#escalado-automÃ¡tico)
7. [Troubleshooting](#troubleshooting)

---

## ğŸ¯ IntroducciÃ³n

Este proyecto implementa un **balanceador de carga en Kubernetes con 2 pods**, diseÃ±ado para:

- âœ… **Alta disponibilidad**: Si un pod falla, el otro sigue sirviendo trÃ¡fico
- âœ… **DistribuciÃ³n de carga**: El trÃ¡fico se reparte equitativamente entre los 2 pods
- âœ… **Actualizaciones sin downtime**: Rolling updates mantienen el servicio disponible
- âœ… **Escalado automÃ¡tico** (opcional): Aumenta pods segÃºn la carga
- âœ… **Health checks**: Detecta y recupera pods problemÃ¡ticos

---

## ğŸ—ï¸ Arquitectura

```
                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                           â”‚   LOADBALANCER SERVICE      â”‚
                           â”‚   (IP Externa: X.X.X.X:80)  â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Algoritmo: Round-Robin          â”‚
                    â”‚   SessionAffinity: None            â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                                   â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   POD 1          â”‚              â”‚   POD 2          â”‚
          â”‚   nginx:alpine   â”‚              â”‚   nginx:alpine   â”‚
          â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚              â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
          â”‚   CPU: 100m-500m â”‚              â”‚   CPU: 100m-500m â”‚
          â”‚   RAM: 64Mi-256Miâ”‚              â”‚   RAM: 64Mi-256Miâ”‚
          â”‚   Port: 80       â”‚              â”‚   Port: 80       â”‚
          â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚              â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
          â”‚   âœ“ Liveness    â”‚              â”‚   âœ“ Liveness    â”‚
          â”‚   âœ“ Readiness   â”‚              â”‚   âœ“ Readiness   â”‚
          â”‚   âœ“ Startup     â”‚              â”‚   âœ“ Startup     â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚                                 â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   NODO 1         â”‚              â”‚   NODO 2         â”‚
          â”‚  (preferido)     â”‚              â”‚  (preferido)     â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flujo de TrÃ¡fico

1. **Cliente** â†’ Hace peticiÃ³n a `LoadBalancerIP:80`
2. **LoadBalancer** â†’ Selecciona un pod (round-robin)
3. **Pod** â†’ Procesa la peticiÃ³n y responde
4. **Si un pod falla**:
   - Readiness probe lo detecta
   - LoadBalancer deja de enviarle trÃ¡fico
   - Todo el trÃ¡fico va al pod saludable
   - Liveness probe reinicia el pod problemÃ¡tico
   - Cuando se recupera, vuelve al balanceo

---

## âš™ï¸ Ajustes Detallados

### 1ï¸âƒ£ RÃ‰PLICAS: 2 Pods

```yaml
spec:
  replicas: 2
```

**Â¿Por quÃ© 2 pods?**
- âœ… **MÃ­nimo para alta disponibilidad**: Si 1 falla, el otro funciona
- âœ… **Balanceo real**: Reparte la carga 50%-50%
- âœ… **Costo-beneficio**: MÃ¡s pods = mÃ¡s recursos consumidos
- âœ… **Escalable**: Con HPA puede crecer a 5 pods automÃ¡ticamente

**Alternativas:**
- **1 pod**: No hay alta disponibilidad ni balanceo
- **3+ pods**: Mayor disponibilidad y distribuciÃ³n, pero mÃ¡s recursos

---

### 2ï¸âƒ£ ESTRATEGIA: RollingUpdate

```yaml
strategy:
  type: RollingUpdate
  rollingUpdate:
    maxSurge: 1         # Permite 1 pod extra durante actualizaciÃ³n
    maxUnavailable: 0   # Siempre al menos 2 pods disponibles
```

**Â¿CÃ³mo funciona el Rolling Update?**

**Escenario: Actualizar de nginx:1.23 a nginx:1.24**

```
Estado Inicial:
[Pod1: v1.23] [Pod2: v1.23]  â† 2 pods funcionando

Paso 1 (maxSurge: 1):
[Pod1: v1.23] [Pod2: v1.23] [Pod3: v1.24]  â† Crea 1 pod nuevo (total: 3)

Paso 2 (espera readiness de Pod3):
[Pod1: v1.23] [Pod2: v1.23] [Pod3: v1.24 âœ“]  â† Pod3 listo

Paso 3 (elimina Pod1):
[Pod2: v1.23] [Pod3: v1.24 âœ“]  â† Ahora 2 pods

Paso 4 (crea Pod4):
[Pod2: v1.23] [Pod3: v1.24 âœ“] [Pod4: v1.24]  â† Crea otro pod nuevo

Paso 5 (espera readiness de Pod4):
[Pod2: v1.23] [Pod3: v1.24 âœ“] [Pod4: v1.24 âœ“]

Paso 6 (elimina Pod2):
[Pod3: v1.24 âœ“] [Pod4: v1.24 âœ“]  â† ActualizaciÃ³n completa

Estado Final:
[Pod3: v1.24] [Pod4: v1.24]  â† 2 pods en nueva versiÃ³n
```

**Ventajas:**
- âœ… **Cero downtime**: Siempre hay pods disponibles
- âœ… **Gradual**: Detecta problemas antes de actualizar todos
- âœ… **Reversible**: FÃ¡cil hacer rollback si algo falla

---

### 3ï¸âƒ£ RECURSOS: Requests y Limits

```yaml
resources:
  requests:
    memory: "64Mi"    # MÃ­nimo garantizado
    cpu: "100m"       # 0.1 CPU
  limits:
    memory: "256Mi"   # MÃ¡ximo permitido
    cpu: "500m"       # 0.5 CPU
```

**Â¿QuÃ© significan?**

| MÃ©trica | Request | Limit | ExplicaciÃ³n |
|---------|---------|-------|-------------|
| **Memoria** | 64Mi | 256Mi | Kubernetes reserva 64Mi. Si usa >256Mi, pod reinicia (OOMKilled) |
| **CPU** | 100m | 500m | Kubernetes reserva 0.1 CPU. Si usa >500m, se limita (throttling) |

**Con 2 pods:**
- **Recursos reservados**: 128Mi RAM, 0.2 CPU (2 Ã— requests)
- **Consumo mÃ¡ximo**: 512Mi RAM, 1.0 CPU (2 Ã— limits)

**Â¿CÃ³mo elegir valores?**

1. **Medir primero**: Desplegar con valores altos, observar consumo real
2. **Requests**: Consumo promedio tÃ­pico
3. **Limits**: Consumo pico mÃ¡ximo + margen 20-30%

**Ejemplo con mÃ©tricas:**
```bash
# Ver consumo real de los pods
kubectl top pods -l app=web-lb

# Resultado ejemplo:
NAME                    CPU   MEMORY
web-lb-pod-1           45m   52Mi
web-lb-pod-2           38m   48Mi

# ConclusiÃ³n: requests (100m/64Mi) son adecuados
```

---

### 4ï¸âƒ£ PROBES: Health Checks

#### A) **Liveness Probe** - Â¿EstÃ¡ vivo?

```yaml
livenessProbe:
  httpGet:
    path: /
    port: 80
  initialDelaySeconds: 15   # Espera 15s antes de empezar
  periodSeconds: 10         # Chequea cada 10s
  failureThreshold: 3       # 3 fallos = reinicia pod
```

**Funcionamiento:**
```
Tiempo  | AcciÃ³n
--------|--------------------------------------------------------
0s      | Pod inicia
15s     | Primera verificaciÃ³n â†’ GET http://pod-ip:80/
25s     | Segunda verificaciÃ³n â†’ GET http://pod-ip:80/
35s     | Tercera verificaciÃ³n â†’ GET http://pod-ip:80/ (FALLA)
45s     | Cuarta verificaciÃ³n (FALLA)
55s     | Quinta verificaciÃ³n (FALLA) â† 3 fallos consecutivos
55s     | âŒ Kubernetes REINICIA el pod
```

**Â¿CuÃ¡ndo usar?**
- âœ… Detectar deadlocks (app bloqueada)
- âœ… Detectar corrupciÃ³n de memoria
- âœ… Detectar app en estado invÃ¡lido

#### B) **Readiness Probe** - Â¿EstÃ¡ listo?

```yaml
readinessProbe:
  httpGet:
    path: /
    port: 80
  initialDelaySeconds: 5
  periodSeconds: 5
  failureThreshold: 3       # 3 fallos = saca del LoadBalancer
```

**Diferencia con Liveness:**
- **Liveness falla** â†’ Reinicia el pod (destructivo)
- **Readiness falla** â†’ Quita del LoadBalancer (no destructivo)

**Ejemplo prÃ¡ctico:**

```
Pod estÃ¡ iniciando, conectando a DB...

Tiempo  | Readiness | LoadBalancer | Estado
--------|-----------|--------------|----------------------------------
0s      | âŒ FAIL   | Sin trÃ¡fico  | App conectando a DB...
5s      | âŒ FAIL   | Sin trÃ¡fico  | TodavÃ­a conectando...
10s     | âœ… OK     | âœ“ Recibe     | Conectado, listo para peticiones
15s     | âœ… OK     | âœ“ Recibe     | Sirviendo trÃ¡fico
20s     | âŒ FAIL   | Sin trÃ¡fico  | DB desconectÃ³ (pero app viva)
25s     | âŒ FAIL   | Sin trÃ¡fico  | Intentando reconectar...
30s     | âœ… OK     | âœ“ Recibe     | Reconectado, vuelve al balanceo
```

**Ventaja:** El pod no se reinicia (conserva conexiones, cache, estado)

#### C) **Startup Probe** - Â¿IniciÃ³ correctamente?

```yaml
startupProbe:
  httpGet:
    path: /
    port: 80
  periodSeconds: 2
  failureThreshold: 15      # 15 Ã— 2s = 30s mÃ¡ximo de inicio
```

**Â¿Para quÃ© sirve?**
- Apps que tardan en iniciar (Java, frameworks pesados)
- Desactiva liveness probe hasta que startup tenga Ã©xito
- Evita que liveness reinicie un pod que estÃ¡ iniciando lentamente

---

### 5ï¸âƒ£ ANTI-AFINIDAD: DistribuciÃ³n en Nodos

```yaml
affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      podAffinityTerm:
        labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - web-lb
        topologyKey: kubernetes.io/hostname
```

**Â¿QuÃ© hace?**
- **PREFIERE** poner cada pod en un nodo diferente
- No es obligatorio (si solo hay 1 nodo, ambos van ahÃ­)

**Escenarios:**

**Cluster con 2+ nodos:**
```
Nodo1: [Pod1: web-lb] â† Primer pod aquÃ­
Nodo2: [Pod2: web-lb] â† Segundo pod aquÃ­ (preferido)
```

**Cluster con 1 nodo:**
```
Nodo1: [Pod1: web-lb] [Pod2: web-lb] â† Ambos aquÃ­ (permitido)
```

**Â¿Por quÃ© es importante?**
- âœ… Si Nodo1 falla, Nodo2 sigue sirviendo
- âœ… Mejor disponibilidad
- âœ… Aislamiento de fallos

**Alternativa obligatoria:**
```yaml
# requiredDuringSchedulingIgnoredDuringExecution (mÃ¡s estricto)
# Si no hay nodos disponibles, el pod NO se crea
```

---

### 6ï¸âƒ£ SERVICE: LoadBalancer

```yaml
type: LoadBalancer
sessionAffinity: None
externalTrafficPolicy: Cluster
```

**Tipos de Service comparados:**

| Tipo | Acceso | IP Externa | Load Balancing |
|------|--------|------------|----------------|
| **ClusterIP** | Solo dentro del cluster | âŒ No | SÃ­ (interno) |
| **NodePort** | NodoIP:Puerto | âŒ No | SÃ­ |
| **LoadBalancer** | IP dedicada | âœ… SÃ­ | âœ… SÃ­ |

**Algoritmo de balanceo: Round-Robin**

```
PeticiÃ³n 1 â†’ Pod1
PeticiÃ³n 2 â†’ Pod2
PeticiÃ³n 3 â†’ Pod1
PeticiÃ³n 4 â†’ Pod2
...
```

**sessionAffinity:**
- **None** (default): Round-robin puro, mejor balanceo
- **ClientIP**: Mismo cliente va siempre al mismo pod (sticky session)

**Â¿CuÃ¡ndo usar sessionAffinity: ClientIP?**
```yaml
sessionAffinity: ClientIP
sessionAffinityConfig:
  clientIP:
    timeoutSeconds: 10800  # 3 horas
```

Usar cuando:
- âœ… App guarda sesiÃ³n en memoria (sin Redis/DB)
- âœ… WebSockets (necesita misma conexiÃ³n)
- âœ… Cargas de archivos largas

**externalTrafficPolicy:**

| Valor | Ventajas | Desventajas |
|-------|----------|-------------|
| **Cluster** | Mejor balanceo, mÃ¡s resiliente | Pierde IP origen del cliente |
| **Local** | Preserva IP cliente, menor latencia | Puede haber balanceo desigual |

---

### 7ï¸âƒ£ HPA: Escalado AutomÃ¡tico

```yaml
minReplicas: 2
maxReplicas: 5
metrics:
- type: Resource
  resource:
    name: cpu
    target:
      averageUtilization: 70  # Escala cuando CPU > 70%
```

**Â¿CÃ³mo escala?**

**FÃ³rmula:**
```
pods_deseados = ceil(pods_actuales Ã— (uso_actual / objetivo))
```

**Ejemplo prÃ¡ctico:**

```
Estado inicial:
- 2 pods
- CPU promedio: 40%
- Objetivo: 70%

CÃ¡lculo: 2 Ã— (40 / 70) = 1.14 â†’ ceil(1.14) = 2 pods
â†’ Sin cambios (dentro del objetivo)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Pico de trÃ¡fico:
- 2 pods
- CPU promedio: 85%
- Objetivo: 70%

CÃ¡lculo: 2 Ã— (85 / 70) = 2.43 â†’ ceil(2.43) = 3 pods
â†’ â¬†ï¸ ESCALA A 3 PODS

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

MÃ¡s trÃ¡fico:
- 3 pods
- CPU promedio: 80%
- Objetivo: 70%

CÃ¡lculo: 3 Ã— (80 / 70) = 3.43 â†’ ceil(3.43) = 4 pods
â†’ â¬†ï¸ ESCALA A 4 PODS

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

TrÃ¡fico baja:
- 4 pods
- CPU promedio: 30%
- Objetivo: 70%

CÃ¡lculo: 4 Ã— (30 / 70) = 1.71 â†’ ceil(1.71) = 2 pods
â†’ â¬‡ï¸ ESCALA A 2 PODS (respeta minReplicas)
```

**Comportamiento de escalado:**

```yaml
behavior:
  scaleUp:
    stabilizationWindowSeconds: 0       # Escala arriba inmediatamente
    policies:
    - type: Percent
      value: 100                        # Puede doblar pods
      periodSeconds: 15

  scaleDown:
    stabilizationWindowSeconds: 300     # Espera 5 min antes de bajar
    policies:
    - type: Pods
      value: 1                          # Baja de 1 en 1
      periodSeconds: 60
```

**Timeline de scale down:**

```
Tiempo | CPU | Pods | AcciÃ³n
-------|-----|------|---------------------------------------
0:00   | 85% | 4    | TrÃ¡fico alto
0:01   | 30% | 4    | TrÃ¡fico baja bruscamente
0:01   | 30% | 4    | HPA detecta, pero espera (stabilization)
1:00   | 30% | 4    | TodavÃ­a esperando...
3:00   | 30% | 4    | TodavÃ­a esperando...
5:00   | 30% | 4    | âœ“ 5 min de estabilidad
5:00   | 30% | 3    | â¬‡ï¸ Remueve 1 pod
6:00   | 30% | 3    | Espera 1 min
6:00   | 30% | 2    | â¬‡ï¸ Remueve otro pod (mÃ­nimo alcanzado)
```

**Â¿Por quÃ© esperar 5 min para bajar?**
- âœ… Evita "flapping" (subir/bajar constantemente)
- âœ… El trÃ¡fico puede subir de nuevo
- âœ… MÃ¡s estable que reactivo

---

### 8ï¸âƒ£ PDB: Pod Disruption Budget

```yaml
minAvailable: 1
```

**Â¿QuÃ© protege?**

Durante operaciones voluntarias (no crashes):
- Drain de nodo (mantenimiento)
- ActualizaciÃ³n de Kubernetes
- Redimensionamiento de cluster

**Ejemplo: Drain de nodo**

```
Sin PDB:
Nodo1: [Pod1] [Pod2]
Nodo2: []

$ kubectl drain nodo1
â†’ Elimina Pod1 y Pod2 simultÃ¡neamente
â†’ âš ï¸ 0 pods disponibles temporalmente
â†’ âš ï¸ DOWNTIME

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Con PDB (minAvailable: 1):
Nodo1: [Pod1] [Pod2]
Nodo2: []

$ kubectl drain nodo1
â†’ Crea Pod3 en Nodo2
â†’ Espera que Pod3 estÃ© ready
â†’ Elimina Pod1
â†’ Crea Pod4 en Nodo2
â†’ Espera que Pod4 estÃ© ready
â†’ Elimina Pod2
â†’ âœ… Siempre al menos 1 pod disponible
â†’ âœ… CERO DOWNTIME
```

---

## ğŸš€ Despliegue

### Paso 1: Crear el Deployment y Service

```bash
# Aplicar la configuraciÃ³n
kubectl apply -f loadbalancer-deployment.yaml

# Verificar deployment
kubectl get deployment web-loadbalancer-deployment

# Verificar pods (deben ser 2)
kubectl get pods -l app=web-lb

# Verificar service
kubectl get service web-loadbalancer-service
```

### Paso 2: Obtener IP del LoadBalancer

```bash
# En Minikube
minikube service web-loadbalancer-service --url

# En cloud (AWS, GCP, Azure)
kubectl get service web-loadbalancer-service
# Espera a que EXTERNAL-IP cambie de <pending> a una IP real
```

### Paso 3: Probar el balanceo

```bash
# Obtener IP del LoadBalancer
LB_IP=$(kubectl get service web-loadbalancer-service -o jsonpath='{.status.loadBalancer.ingress[0].ip}')

# Hacer mÃºltiples peticiones
for i in {1..10}; do
  curl http://$LB_IP
  echo "---"
done
```

---

## âœ… Pruebas y VerificaciÃ³n

### 1ï¸âƒ£ Verificar que hay 2 pods

```bash
kubectl get pods -l app=web-lb

# Salida esperada:
NAME                                            READY   STATUS    RESTARTS   AGE
web-loadbalancer-deployment-5d4f8b6c9d-abc12   1/1     Running   0          2m
web-loadbalancer-deployment-5d4f8b6c9d-def34   1/1     Running   0          2m
```

### 2ï¸âƒ£ Verificar distribuciÃ³n en nodos

```bash
kubectl get pods -l app=web-lb -o wide

# Ver columna NODE - idealmente en nodos diferentes
```

### 3ï¸âƒ£ Probar alta disponibilidad

```bash
# Eliminar un pod manualmente
kubectl delete pod <nombre-de-un-pod>

# Kubernetes debe:
# 1. Crear un pod nuevo inmediatamente
# 2. El LoadBalancer sigue funcionando con el otro pod
# 3. Cuando el nuevo pod estÃ© listo, vuelve al balanceo

# Verificar que siempre hay 2 pods
watch kubectl get pods -l app=web-lb
```

### 4ï¸âƒ£ Ver logs de ambos pods

```bash
# Logs en tiempo real de ambos pods
kubectl logs -l app=web-lb --follow --prefix
```

### 5ï¸âƒ£ Simular carga para HPA

```bash
# Generar trÃ¡fico (instala 'hey' primero)
# https://github.com/rakyll/hey
LB_IP=$(kubectl get service web-loadbalancer-service -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
hey -z 5m -c 100 http://$LB_IP

# Observar escalado
watch kubectl get hpa web-loadbalancer-hpa
watch kubectl get pods -l app=web-lb
```

### 6ï¸âƒ£ Verificar health checks

```bash
# Ver eventos de probes
kubectl describe pod <nombre-pod> | grep -A 5 "Liveness\|Readiness\|Startup"

# Ver eventos de reinicios
kubectl get events --sort-by='.lastTimestamp' | grep -i "unhealthy\|failed"
```

---

## ğŸ“ˆ Escalado AutomÃ¡tico

### Prerequisito: Metrics Server

```bash
# Verificar si metrics-server estÃ¡ instalado
kubectl get deployment metrics-server -n kube-system

# Si no estÃ¡, instalarlo
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

# En Minikube
minikube addons enable metrics-server
```

### Monitorear HPA

```bash
# Ver estado del HPA
kubectl get hpa web-loadbalancer-hpa

# Salida ejemplo:
NAME                      REFERENCE                                    TARGETS   MINPODS   MAXPODS   REPLICAS
web-loadbalancer-hpa      Deployment/web-loadbalancer-deployment       45%/70%   2         5         2

# Ver eventos de escalado
kubectl describe hpa web-loadbalancer-hpa
```

### Probar escalado manual

```bash
# Escalar manualmente (HPA lo ajustarÃ¡ despuÃ©s)
kubectl scale deployment web-loadbalancer-deployment --replicas=4

# HPA volverÃ¡ al nÃºmero Ã³ptimo segÃºn mÃ©tricas
```

---

## ğŸ”§ Troubleshooting

### âŒ Problema: LoadBalancer en estado <pending>

**SÃ­ntoma:**
```bash
kubectl get svc web-loadbalancer-service
# EXTERNAL-IP: <pending>
```

**Causa:** EstÃ¡s en Minikube o cluster sin proveedor de LoadBalancer

**SoluciÃ³n:**
```bash
# En Minikube, usar tunnel
minikube tunnel
# Deja esta terminal abierta

# O usa minikube service
minikube service web-loadbalancer-service
```

---

### âŒ Problema: Pods no se distribuyen en nodos diferentes

**Verificar:**
```bash
kubectl get pods -l app=web-lb -o wide
# Ver columna NODE
```

**Causa:** Solo hay 1 nodo o la anti-afinidad es "preferred" (no obligatoria)

**Verificar nodos:**
```bash
kubectl get nodes
```

**SoluciÃ³n:** La configuraciÃ³n actual es "preferred", estÃ¡ bien tener ambos en el mismo nodo

---

### âŒ Problema: HPA no escala

**SÃ­ntoma:**
```bash
kubectl get hpa
# TARGETS: <unknown>
```

**Causa:** Metrics server no estÃ¡ instalado

**Verificar:**
```bash
kubectl top nodes
kubectl top pods

# Si da error, instalar metrics-server
minikube addons enable metrics-server
```

---

### âŒ Problema: Pod se reinicia constantemente (CrashLoopBackOff)

**Verificar logs:**
```bash
kubectl logs <nombre-pod>
kubectl describe pod <nombre-pod>
```

**Causas comunes:**
1. Imagen incorrecta
2. Liveness probe muy agresivo
3. Recursos insuficientes (OOMKilled)

**SoluciÃ³n temporal:**
```bash
# Aumentar initialDelaySeconds del liveness probe
# O aumentar limits de memoria
```

---

### âŒ Problema: TrÃ¡fico solo va a un pod

**Verificar endpoints:**
```bash
kubectl get endpoints web-loadbalancer-service

# Debe mostrar IPs de ambos pods
# Si solo hay 1 IP, el otro pod no pasa readiness probe
```

**Verificar readiness:**
```bash
kubectl get pods -l app=web-lb

# READY debe ser 1/1 en ambos
# Si es 0/1, ver logs y eventos
```

---

## ğŸ“Š Monitoreo Continuo

### Dashboard de Kubernetes

```bash
# En Minikube
minikube dashboard
```

### Comandos Ãºtiles

```bash
# Estado general
kubectl get all -l app=web-lb

# Recursos consumidos
kubectl top pods -l app=web-lb

# Eventos recientes
kubectl get events --sort-by='.lastTimestamp' | grep web-lb

# Watch en tiempo real
watch kubectl get pods,svc,hpa -l app=web-lb
```

---

## ğŸ“ Resumen de Conceptos Clave

| Concepto | Valor | PropÃ³sito |
|----------|-------|-----------|
| **RÃ©plicas** | 2 | Alta disponibilidad + balanceo |
| **maxSurge** | 1 | Permite 3 pods durante update |
| **maxUnavailable** | 0 | Siempre 2+ pods disponibles |
| **CPU request** | 100m | Reserva 0.1 CPU por pod |
| **CPU limit** | 500m | MÃ¡ximo 0.5 CPU por pod |
| **Memory request** | 64Mi | Reserva 64MB por pod |
| **Memory limit** | 256Mi | MÃ¡ximo 256MB por pod |
| **Liveness** | 15s delay | Reinicia si falla 3 veces |
| **Readiness** | 5s delay | Quita del LB si falla 3 veces |
| **HPA min** | 2 | MÃ­nimo de pods |
| **HPA max** | 5 | MÃ¡ximo de pods |
| **HPA target** | 70% CPU | Escala cuando excede 70% |
| **PDB** | minAvailable: 1 | Al menos 1 pod durante drain |

---

## ğŸ”— PrÃ³ximos Pasos

1. **Monitoreo avanzado**: Integrar Prometheus + Grafana
2. **Logs centralizados**: ELK Stack o Loki
3. **Ingress**: Agregar Ingress Controller para mÃºltiples servicios
4. **TLS/HTTPS**: Certificados SSL con cert-manager
5. **CI/CD**: Automatizar despliegues con GitOps (ArgoCD/Flux)

---

## ğŸ“š Referencias

- [Kubernetes Deployments](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/)
- [Kubernetes Services](https://kubernetes.io/docs/concepts/services-networking/service/)
- [Horizontal Pod Autoscaler](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)
- [Pod Disruption Budgets](https://kubernetes.io/docs/concepts/workloads/pods/disruptions/)
- [Configure Liveness, Readiness and Startup Probes](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/)
